2023-12-16 21:01:47,402 - Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-12-16 21:01:47,955 - [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-12-16 21:08:49,251 - Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-12-16 21:08:49,820 - [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-12-16 21:09:26,908 - epoch 1, train_loss: 0.7564, train_accuracy: 0.6205
2023-12-16 21:09:35,141 - epoch 1, val_loss: 0.6301, val_accuracy: 0.6682
2023-12-16 21:09:58,765 - epoch 2, train_loss: 0.6739, train_accuracy: 0.6682
2023-12-16 21:10:06,997 - epoch 2, val_loss: 0.6337, val_accuracy: 0.6659
2023-12-16 21:10:30,614 - epoch 3, train_loss: 0.6285, train_accuracy: 0.6682
2023-12-16 21:10:38,749 - epoch 3, val_loss: 0.5528, val_accuracy: 0.7023
2023-12-16 21:11:02,314 - epoch 4, train_loss: 0.5304, train_accuracy: 0.7568
2023-12-16 21:11:10,190 - epoch 4, val_loss: 0.8058, val_accuracy: 0.6750
2023-12-16 21:11:33,690 - epoch 5, train_loss: 0.5018, train_accuracy: 0.7682
2023-12-16 21:11:41,807 - epoch 5, val_loss: 0.4927, val_accuracy: 0.7614
2023-12-16 21:12:05,160 - epoch 6, train_loss: 0.3274, train_accuracy: 0.8523
2023-12-16 21:12:13,205 - epoch 6, val_loss: 0.2812, val_accuracy: 0.8432
2023-12-16 21:12:36,678 - epoch 7, train_loss: 0.2022, train_accuracy: 0.9182
2023-12-16 21:12:44,735 - epoch 7, val_loss: 0.3216, val_accuracy: 0.8636
2023-12-16 21:13:08,115 - epoch 8, train_loss: 0.1331, train_accuracy: 0.9545
2023-12-16 21:13:16,179 - epoch 8, val_loss: 0.1786, val_accuracy: 0.9091
2023-12-16 21:13:39,619 - epoch 9, train_loss: 0.0579, train_accuracy: 0.9795
2023-12-16 21:13:47,796 - epoch 9, val_loss: 0.2441, val_accuracy: 0.8977
2023-12-16 21:14:10,240 - epoch 10, train_loss: 0.0532, train_accuracy: 0.9818
2023-12-16 21:14:18,398 - epoch 10, val_loss: 0.1847, val_accuracy: 0.9159
2023-12-16 21:14:20,332 - 1 fold, loss: 0.1847, accuracy: 0.9159
2023-12-16 21:14:43,579 - epoch 1, train_loss: 0.6455, train_accuracy: 0.6682
2023-12-16 21:14:51,785 - epoch 1, val_loss: 0.3630, val_accuracy: 0.8523
2023-12-16 21:15:15,434 - epoch 2, train_loss: 0.3998, train_accuracy: 0.8364
2023-12-16 21:15:23,221 - epoch 2, val_loss: 0.5475, val_accuracy: 0.6886
2023-12-16 21:15:46,719 - epoch 3, train_loss: 0.3017, train_accuracy: 0.8545
2023-12-16 21:15:55,070 - epoch 3, val_loss: 0.2347, val_accuracy: 0.9068
2023-12-16 21:16:18,274 - epoch 4, train_loss: 0.2124, train_accuracy: 0.9159
2023-12-16 21:16:25,468 - epoch 4, val_loss: 0.2437, val_accuracy: 0.8545
2023-12-16 21:16:44,796 - epoch 5, train_loss: 0.0978, train_accuracy: 0.9659
2023-12-16 21:16:51,860 - epoch 5, val_loss: 0.1292, val_accuracy: 0.9432
2023-12-16 21:17:11,127 - epoch 6, train_loss: 0.0468, train_accuracy: 0.9841
2023-12-16 21:17:18,219 - epoch 6, val_loss: 0.1420, val_accuracy: 0.9455
2023-12-16 21:17:37,507 - epoch 7, train_loss: 0.0189, train_accuracy: 0.9909
2023-12-16 21:17:44,577 - epoch 7, val_loss: 0.1507, val_accuracy: 0.9545
2023-12-16 21:18:03,851 - epoch 8, train_loss: 0.0089, train_accuracy: 0.9955
2023-12-16 21:18:10,930 - epoch 8, val_loss: 0.1922, val_accuracy: 0.9364
2023-12-16 21:18:30,273 - epoch 9, train_loss: 0.0029, train_accuracy: 1.0000
2023-12-16 21:18:37,340 - epoch 9, val_loss: 0.2025, val_accuracy: 0.9250
2023-12-16 21:18:56,667 - epoch 10, train_loss: 0.0023, train_accuracy: 1.0000
2023-12-16 21:19:03,776 - epoch 10, val_loss: 0.2162, val_accuracy: 0.9227
2023-12-16 21:19:05,285 - 2 fold, loss: 0.2162, accuracy: 0.9227
2023-12-16 21:19:24,016 - epoch 1, train_loss: 0.6641, train_accuracy: 0.6591
2023-12-16 21:19:31,075 - epoch 1, val_loss: 0.6736, val_accuracy: 0.5750
2023-12-16 21:19:50,848 - epoch 2, train_loss: 0.6472, train_accuracy: 0.6841
2023-12-16 21:19:58,625 - epoch 2, val_loss: 0.6779, val_accuracy: 0.5636
2023-12-16 21:20:21,153 - epoch 3, train_loss: 0.5689, train_accuracy: 0.7182
2023-12-16 21:20:28,865 - epoch 3, val_loss: 0.5587, val_accuracy: 0.7295
2023-12-16 21:20:49,680 - epoch 4, train_loss: 0.5581, train_accuracy: 0.7114
2023-12-16 21:20:57,120 - epoch 4, val_loss: 0.5799, val_accuracy: 0.6477
2023-12-16 21:21:19,351 - epoch 5, train_loss: 0.3647, train_accuracy: 0.8500
2023-12-16 21:21:26,511 - epoch 5, val_loss: 0.4604, val_accuracy: 0.8136
2023-12-16 21:21:46,571 - epoch 6, train_loss: 0.2995, train_accuracy: 0.8773
2023-12-16 21:21:55,060 - epoch 6, val_loss: 0.6066, val_accuracy: 0.7727
2023-12-16 21:22:29,090 - epoch 7, train_loss: 0.2081, train_accuracy: 0.9227
2023-12-16 21:22:38,850 - epoch 7, val_loss: 0.3527, val_accuracy: 0.8886
2023-12-16 21:23:14,563 - epoch 8, train_loss: 0.1354, train_accuracy: 0.9432
2023-12-16 21:23:24,068 - epoch 8, val_loss: 0.2320, val_accuracy: 0.9091
2023-12-16 21:23:57,511 - epoch 9, train_loss: 0.0742, train_accuracy: 0.9705
2023-12-16 21:24:06,133 - epoch 9, val_loss: 0.2472, val_accuracy: 0.9000
2023-12-16 21:24:39,932 - epoch 10, train_loss: 0.0625, train_accuracy: 0.9818
2023-12-16 21:24:49,664 - epoch 10, val_loss: 0.2831, val_accuracy: 0.9000
2023-12-16 21:24:51,464 - 3 fold, loss: 0.2831, accuracy: 0.9000
2023-12-16 21:25:24,871 - epoch 1, train_loss: 0.7926, train_accuracy: 0.6159
2023-12-16 21:25:33,091 - epoch 1, val_loss: 0.6343, val_accuracy: 0.7159
2023-12-16 21:25:55,425 - epoch 2, train_loss: 0.6881, train_accuracy: 0.6591
2023-12-16 21:26:02,989 - epoch 2, val_loss: 0.6007, val_accuracy: 0.7068
2023-12-16 21:26:24,048 - epoch 3, train_loss: 0.6163, train_accuracy: 0.6591
2023-12-16 21:26:31,108 - epoch 3, val_loss: 0.5001, val_accuracy: 0.7864
2023-12-16 21:26:50,397 - epoch 4, train_loss: 0.6338, train_accuracy: 0.7000
2023-12-16 21:26:57,425 - epoch 4, val_loss: 0.4794, val_accuracy: 0.7500
2023-12-16 21:27:16,742 - epoch 5, train_loss: 0.4571, train_accuracy: 0.7727
2023-12-16 21:27:23,921 - epoch 5, val_loss: 0.2126, val_accuracy: 0.9455
2023-12-16 21:27:43,296 - epoch 6, train_loss: 0.2270, train_accuracy: 0.9068
2023-12-16 21:27:51,266 - epoch 6, val_loss: 0.1802, val_accuracy: 0.9182
2023-12-16 21:28:12,404 - epoch 7, train_loss: 0.1480, train_accuracy: 0.9273
2023-12-16 21:28:20,162 - epoch 7, val_loss: 0.0777, val_accuracy: 0.9727
2023-12-16 21:28:39,953 - epoch 8, train_loss: 0.1128, train_accuracy: 0.9523
2023-12-16 21:28:47,480 - epoch 8, val_loss: 0.0874, val_accuracy: 0.9818
2023-12-16 21:29:09,989 - epoch 9, train_loss: 0.0525, train_accuracy: 0.9841
2023-12-16 21:29:17,331 - epoch 9, val_loss: 0.0768, val_accuracy: 0.9727
2023-12-16 21:29:38,682 - epoch 10, train_loss: 0.0355, train_accuracy: 0.9909
2023-12-16 21:29:46,266 - epoch 10, val_loss: 0.0829, val_accuracy: 0.9727
2023-12-16 21:29:47,873 - 4 fold, loss: 0.0829, accuracy: 0.9727
2023-12-16 21:30:09,444 - epoch 1, train_loss: 0.7347, train_accuracy: 0.6205
2023-12-16 21:30:17,053 - epoch 1, val_loss: 0.5528, val_accuracy: 0.7205
2023-12-16 21:30:39,256 - epoch 2, train_loss: 0.5061, train_accuracy: 0.7591
2023-12-16 21:30:46,842 - epoch 2, val_loss: 0.9097, val_accuracy: 0.5841
2023-12-16 21:31:07,685 - epoch 3, train_loss: 0.4677, train_accuracy: 0.7955
2023-12-16 21:31:15,332 - epoch 3, val_loss: 0.6611, val_accuracy: 0.7068
2023-12-16 21:31:37,570 - epoch 4, train_loss: 0.3148, train_accuracy: 0.8591
2023-12-16 21:31:45,034 - epoch 4, val_loss: 0.2497, val_accuracy: 0.8705
2023-12-16 21:32:06,078 - epoch 5, train_loss: 0.1642, train_accuracy: 0.9159
2023-12-16 21:32:13,569 - epoch 5, val_loss: 0.0960, val_accuracy: 0.9727
2023-12-16 21:32:35,874 - epoch 6, train_loss: 0.1295, train_accuracy: 0.9500
2023-12-16 21:32:43,439 - epoch 6, val_loss: 0.0919, val_accuracy: 0.9614
2023-12-16 21:33:05,728 - epoch 7, train_loss: 0.0820, train_accuracy: 0.9682
2023-12-16 21:33:13,356 - epoch 7, val_loss: 0.0554, val_accuracy: 0.9727
2023-12-16 21:33:35,612 - epoch 8, train_loss: 0.0301, train_accuracy: 0.9886
2023-12-16 21:33:43,156 - epoch 8, val_loss: 0.0807, val_accuracy: 0.9614
2023-12-16 21:34:05,315 - epoch 9, train_loss: 0.0345, train_accuracy: 0.9909
2023-12-16 21:34:12,875 - epoch 9, val_loss: 0.1040, val_accuracy: 0.9455
2023-12-16 21:34:35,169 - epoch 10, train_loss: 0.0290, train_accuracy: 0.9886
2023-12-16 21:34:42,669 - epoch 10, val_loss: 0.0861, val_accuracy: 0.9636
2023-12-16 21:34:44,247 - 5 fold, loss: 0.0861, accuracy: 0.9636
2023-12-16 21:34:56,961 - test_loss: 0.1005, test_accuracy: 0.9643
2023-12-16 21:34:56,961 - test set evaluate, loss: 0.1005, accuracy: 0.9643
