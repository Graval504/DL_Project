2023-12-16 23:36:58,855 - Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg_in21k)
2023-12-16 23:36:59,417 - [timm/vit_base_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2023-12-16 23:47:41,167 - epoch 1, train_loss: 0.7359, train_accuracy: 0.7591
2023-12-16 23:48:37,359 - epoch 1, val_loss: 0.1131, val_accuracy: 0.9631
2023-12-16 23:58:25,278 - epoch 2, train_loss: 0.1801, train_accuracy: 0.9397
2023-12-16 23:59:23,063 - epoch 2, val_loss: 0.1199, val_accuracy: 0.9586
2023-12-17 00:08:32,138 - epoch 3, train_loss: 0.1154, train_accuracy: 0.9597
2023-12-17 00:09:23,753 - epoch 3, val_loss: 0.0838, val_accuracy: 0.9708
2023-12-17 00:18:35,838 - epoch 4, train_loss: 0.0802, train_accuracy: 0.9734
2023-12-17 00:19:33,216 - epoch 4, val_loss: 0.1160, val_accuracy: 0.9610
2023-12-17 00:29:19,857 - epoch 5, train_loss: 0.0492, train_accuracy: 0.9848
2023-12-17 00:30:19,020 - epoch 5, val_loss: 0.0806, val_accuracy: 0.9741
2023-12-17 00:39:47,916 - epoch 6, train_loss: 0.0264, train_accuracy: 0.9915
2023-12-17 00:40:41,036 - epoch 6, val_loss: 0.0292, val_accuracy: 0.9898
2023-12-17 00:49:29,537 - epoch 7, train_loss: 0.0144, train_accuracy: 0.9953
2023-12-17 00:50:25,547 - epoch 7, val_loss: 0.0330, val_accuracy: 0.9891
2023-12-17 00:59:38,413 - epoch 8, train_loss: 0.0061, train_accuracy: 0.9979
2023-12-17 01:00:31,995 - epoch 8, val_loss: 0.0217, val_accuracy: 0.9942
2023-12-17 01:09:53,080 - epoch 9, train_loss: 0.0018, train_accuracy: 0.9995
2023-12-17 01:10:47,040 - epoch 9, val_loss: 0.0170, val_accuracy: 0.9939
2023-12-17 01:19:58,451 - epoch 10, train_loss: 0.0005, train_accuracy: 0.9998
2023-12-17 01:20:53,531 - epoch 10, val_loss: 0.0123, val_accuracy: 0.9964
2023-12-17 01:21:31,880 - epoch 1, train_loss: 0.3826, train_accuracy: 0.8295
2023-12-17 01:21:39,456 - epoch 1, val_loss: 0.3354, val_accuracy: 0.8773
2023-12-17 01:21:59,812 - epoch 2, train_loss: 0.2031, train_accuracy: 0.9159
2023-12-17 01:22:07,444 - epoch 2, val_loss: 0.2887, val_accuracy: 0.9068
2023-12-17 01:22:27,752 - epoch 3, train_loss: 0.1115, train_accuracy: 0.9545
2023-12-17 01:22:35,807 - epoch 3, val_loss: 0.2817, val_accuracy: 0.8705
2023-12-17 01:22:55,996 - epoch 4, train_loss: 0.0483, train_accuracy: 0.9841
2023-12-17 01:23:03,663 - epoch 4, val_loss: 0.3164, val_accuracy: 0.8909
2023-12-17 01:23:24,947 - epoch 5, train_loss: 0.0448, train_accuracy: 0.9864
2023-12-17 01:23:32,684 - epoch 5, val_loss: 0.3395, val_accuracy: 0.9068
2023-12-17 01:23:53,651 - epoch 6, train_loss: 0.0429, train_accuracy: 0.9886
2023-12-17 01:24:02,008 - epoch 6, val_loss: 0.2787, val_accuracy: 0.9159
2023-12-17 01:24:25,225 - epoch 7, train_loss: 0.0096, train_accuracy: 0.9977
2023-12-17 01:24:33,476 - epoch 7, val_loss: 0.3247, val_accuracy: 0.8864
2023-12-17 01:24:56,941 - epoch 8, train_loss: 0.0056, train_accuracy: 1.0000
2023-12-17 01:25:05,367 - epoch 8, val_loss: 0.3120, val_accuracy: 0.9091
2023-12-17 01:25:26,998 - epoch 9, train_loss: 0.0051, train_accuracy: 1.0000
2023-12-17 01:25:34,649 - epoch 9, val_loss: 0.3203, val_accuracy: 0.9068
2023-12-17 01:25:56,805 - epoch 10, train_loss: 0.0073, train_accuracy: 1.0000
2023-12-17 01:26:04,905 - epoch 10, val_loss: 0.3160, val_accuracy: 0.9091
2023-12-17 01:26:06,470 - 1 fold, loss: 0.3160, accuracy: 0.9091
2023-12-17 01:26:28,023 - epoch 1, train_loss: 0.4516, train_accuracy: 0.8023
2023-12-17 01:26:36,195 - epoch 1, val_loss: 0.2172, val_accuracy: 0.9000
2023-12-17 01:26:58,821 - epoch 2, train_loss: 0.2234, train_accuracy: 0.9068
2023-12-17 01:27:07,337 - epoch 2, val_loss: 0.1813, val_accuracy: 0.9000
2023-12-17 01:27:27,946 - epoch 3, train_loss: 0.1279, train_accuracy: 0.9432
2023-12-17 01:27:36,031 - epoch 3, val_loss: 0.1564, val_accuracy: 0.9250
2023-12-17 01:27:56,310 - epoch 4, train_loss: 0.0612, train_accuracy: 0.9750
2023-12-17 01:28:03,969 - epoch 4, val_loss: 0.2341, val_accuracy: 0.9182
2023-12-17 01:28:24,596 - epoch 5, train_loss: 0.0258, train_accuracy: 0.9932
2023-12-17 01:28:32,614 - epoch 5, val_loss: 0.1715, val_accuracy: 0.9227
2023-12-17 01:28:54,962 - epoch 6, train_loss: 0.0172, train_accuracy: 0.9955
2023-12-17 01:29:03,137 - epoch 6, val_loss: 0.2157, val_accuracy: 0.9250
2023-12-17 01:29:24,594 - epoch 7, train_loss: 0.0193, train_accuracy: 0.9909
2023-12-17 01:29:32,788 - epoch 7, val_loss: 0.3512, val_accuracy: 0.9045
2023-12-17 01:29:54,016 - epoch 8, train_loss: 0.0121, train_accuracy: 0.9955
2023-12-17 01:30:02,033 - epoch 8, val_loss: 0.1855, val_accuracy: 0.9182
2023-12-17 01:30:22,353 - epoch 9, train_loss: 0.0054, train_accuracy: 1.0000
2023-12-17 01:30:30,495 - epoch 9, val_loss: 0.1894, val_accuracy: 0.9136
2023-12-17 01:30:53,273 - epoch 10, train_loss: 0.0071, train_accuracy: 0.9977
2023-12-17 01:31:02,405 - epoch 10, val_loss: 0.1912, val_accuracy: 0.9136
2023-12-17 01:31:04,177 - 2 fold, loss: 0.1912, accuracy: 0.9136
2023-12-17 01:31:24,258 - epoch 1, train_loss: 0.4309, train_accuracy: 0.7886
2023-12-17 01:31:32,590 - epoch 1, val_loss: 0.2629, val_accuracy: 0.8977
2023-12-17 01:31:55,316 - epoch 2, train_loss: 0.2062, train_accuracy: 0.9136
2023-12-17 01:32:04,044 - epoch 2, val_loss: 0.2168, val_accuracy: 0.9068
2023-12-17 01:32:27,038 - epoch 3, train_loss: 0.1210, train_accuracy: 0.9591
2023-12-17 01:32:35,990 - epoch 3, val_loss: 0.2156, val_accuracy: 0.9091
2023-12-17 01:32:57,566 - epoch 4, train_loss: 0.0487, train_accuracy: 0.9818
2023-12-17 01:33:05,760 - epoch 4, val_loss: 0.2068, val_accuracy: 0.9250
2023-12-17 01:33:28,514 - epoch 5, train_loss: 0.0457, train_accuracy: 0.9909
2023-12-17 01:33:37,573 - epoch 5, val_loss: 0.2000, val_accuracy: 0.9273
2023-12-17 01:33:58,857 - epoch 6, train_loss: 0.0289, train_accuracy: 0.9864
2023-12-17 01:34:06,937 - epoch 6, val_loss: 0.1987, val_accuracy: 0.9455
2023-12-17 01:34:28,280 - epoch 7, train_loss: 0.0107, train_accuracy: 0.9977
2023-12-17 01:34:36,539 - epoch 7, val_loss: 0.1956, val_accuracy: 0.9455
2023-12-17 01:34:57,355 - epoch 8, train_loss: 0.0103, train_accuracy: 0.9977
2023-12-17 01:35:05,594 - epoch 8, val_loss: 0.1813, val_accuracy: 0.9364
2023-12-17 01:35:27,580 - epoch 9, train_loss: 0.0138, train_accuracy: 0.9977
2023-12-17 01:35:35,673 - epoch 9, val_loss: 0.1798, val_accuracy: 0.9455
2023-12-17 01:35:56,938 - epoch 10, train_loss: 0.0056, train_accuracy: 1.0000
2023-12-17 01:36:05,029 - epoch 10, val_loss: 0.1857, val_accuracy: 0.9341
2023-12-17 01:36:06,586 - 3 fold, loss: 0.1857, accuracy: 0.9341
2023-12-17 01:36:27,064 - epoch 1, train_loss: 0.4147, train_accuracy: 0.8136
2023-12-17 01:36:35,248 - epoch 1, val_loss: 0.2945, val_accuracy: 0.8818
2023-12-17 01:36:56,562 - epoch 2, train_loss: 0.1942, train_accuracy: 0.9364
2023-12-17 01:37:05,138 - epoch 2, val_loss: 0.2434, val_accuracy: 0.8955
2023-12-17 01:37:26,591 - epoch 3, train_loss: 0.0857, train_accuracy: 0.9773
2023-12-17 01:37:34,692 - epoch 3, val_loss: 0.1987, val_accuracy: 0.9182
2023-12-17 01:37:56,767 - epoch 4, train_loss: 0.0633, train_accuracy: 0.9727
2023-12-17 01:38:04,881 - epoch 4, val_loss: 0.3099, val_accuracy: 0.9000
2023-12-17 01:38:27,767 - epoch 5, train_loss: 0.0375, train_accuracy: 0.9841
2023-12-17 01:38:36,428 - epoch 5, val_loss: 0.1832, val_accuracy: 0.9341
2023-12-17 01:39:00,069 - epoch 6, train_loss: 0.0174, train_accuracy: 0.9932
2023-12-17 01:39:08,745 - epoch 6, val_loss: 0.1507, val_accuracy: 0.9136
2023-12-17 01:39:32,374 - epoch 7, train_loss: 0.0083, train_accuracy: 0.9977
2023-12-17 01:39:40,987 - epoch 7, val_loss: 0.2290, val_accuracy: 0.9250
2023-12-17 01:40:04,757 - epoch 8, train_loss: 0.0064, train_accuracy: 0.9977
2023-12-17 01:40:13,243 - epoch 8, val_loss: 0.1802, val_accuracy: 0.9364
2023-12-17 01:40:37,013 - epoch 9, train_loss: 0.0042, train_accuracy: 0.9977
2023-12-17 01:40:45,631 - epoch 9, val_loss: 0.1881, val_accuracy: 0.9364
2023-12-17 01:41:09,017 - epoch 10, train_loss: 0.0062, train_accuracy: 0.9977
2023-12-17 01:41:17,489 - epoch 10, val_loss: 0.1905, val_accuracy: 0.9273
2023-12-17 01:41:19,183 - 4 fold, loss: 0.1905, accuracy: 0.9273
2023-12-17 01:41:42,216 - epoch 1, train_loss: 0.3793, train_accuracy: 0.8273
2023-12-17 01:41:50,458 - epoch 1, val_loss: 0.3173, val_accuracy: 0.8409
2023-12-17 01:42:13,989 - epoch 2, train_loss: 0.1848, train_accuracy: 0.9227
2023-12-17 01:42:22,891 - epoch 2, val_loss: 0.2463, val_accuracy: 0.8773
2023-12-17 01:42:46,383 - epoch 3, train_loss: 0.1135, train_accuracy: 0.9636
2023-12-17 01:42:54,875 - epoch 3, val_loss: 0.2330, val_accuracy: 0.8909
2023-12-17 01:43:18,268 - epoch 4, train_loss: 0.0900, train_accuracy: 0.9659
2023-12-17 01:43:27,010 - epoch 4, val_loss: 0.4814, val_accuracy: 0.8682
2023-12-17 01:43:49,478 - epoch 5, train_loss: 0.0500, train_accuracy: 0.9886
2023-12-17 01:43:58,318 - epoch 5, val_loss: 0.2588, val_accuracy: 0.8727
2023-12-17 01:44:22,525 - epoch 6, train_loss: 0.0136, train_accuracy: 1.0000
2023-12-17 01:44:31,194 - epoch 6, val_loss: 0.2859, val_accuracy: 0.8614
2023-12-17 01:44:55,377 - epoch 7, train_loss: 0.0104, train_accuracy: 1.0000
2023-12-17 01:45:04,403 - epoch 7, val_loss: 0.3323, val_accuracy: 0.8727
2023-12-17 01:45:28,719 - epoch 8, train_loss: 0.0062, train_accuracy: 1.0000
2023-12-17 01:45:37,407 - epoch 8, val_loss: 0.3218, val_accuracy: 0.8682
2023-12-17 01:46:01,383 - epoch 9, train_loss: 0.0057, train_accuracy: 1.0000
2023-12-17 01:46:10,029 - epoch 9, val_loss: 0.3148, val_accuracy: 0.8614
2023-12-17 01:46:34,060 - epoch 10, train_loss: 0.0049, train_accuracy: 1.0000
2023-12-17 01:46:42,908 - epoch 10, val_loss: 0.3249, val_accuracy: 0.8591
2023-12-17 01:46:44,654 - 5 fold, loss: 0.3249, accuracy: 0.8591
2023-12-17 01:46:57,774 - test set evaluate, loss: 0.1867, accuracy: 0.9500
